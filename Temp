    n = state.shape[0]
    eps = 1e-8
    
    # Tuned parameters
    k_goal = 3.0
    k_pair = 0.008
    k_obs  = 0.02
    k_wall = 0.05
    
    V = np.zeros_like(state)
    
    # 1. GOAL ATTRACTION - SIMPLIFIED
    to_target = targets - state
    dist_to_target = np.linalg.norm(to_target, axis=1) + eps
    
    # Simple linear attraction - stronger when farther
    for i in range(n):
        V[i] += k_goal * to_target[i] / (dist_to_target[i]**0.7)
    
    # 2. ROBOT-ROBOT REPULSION - SIMPLIFIED
    for i in range(n):
        for j in range(i+1, n):
            diff = state[i] - state[j]
            d = np.linalg.norm(diff) + eps
            min_dist = 2 * r
            
            if d < 2.2 * min_dist:
                # Simple inverse square repulsion
                rep_strength = k_pair / ((d - min_dist + 0.02)**2)
                rep = rep_strength * diff / d
                V[i] += rep
                V[j] -= rep
    
    # 3. OBSTACLE AVOIDANCE - SIMPLIFIED
    for obs in obstacles:
        center = np.array(obs["center"])
        radius = obs["radius"]
        
        for i in range(n):
            diff = state[i] - center
            d = np.linalg.norm(diff) + eps
            min_clearance = radius + r
            
            if d < min_clearance + 0.1:
                rep_strength = k_obs / ((d - min_clearance + 0.02)**2)
                V[i] += rep_strength * diff / d
    
    # 4. BOUNDARY REPULSION
    for i in range(n):
        norm = np.linalg.norm(state[i])
        if norm > 0.9:  # Start pushing back earlier
            to_center = -state[i] / (norm + eps)
            V[i] += k_wall * (norm - 0.9) * to_center
    
    # 5. NORMALIZE
    norms = np.linalg.norm(V, axis=1, keepdims=True) + eps
    V = V / norms
    
    return V
